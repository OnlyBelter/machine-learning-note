# -*- coding: utf-8 -*-
"""PyTorch Transformer for a Sequence-to-Sequence Task

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tBsnYQojlJTRCSnnlE0xUbJ-v7WEPK7d
"""

import torch
import torch.nn as nn
import torch.optim as optim
import math
import numpy as np

# ======================================================================================
# 1. Positional Encoding
# ======================================================================================
# The Transformer architecture does not inherently understand the order of a sequence.
# Positional Encoding adds information about the position of each token in the sequence.
# It uses sine and cosine functions of different frequencies.

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        """
        Args:
            d_model (int): The dimensionality of the model's embeddings.
            max_len (int): The maximum possible length of a sequence.
        """
        super(PositionalEncoding, self).__init__()

        # Create a matrix of shape (max_len, d_model) to hold the positional encodings.
        pe = torch.zeros(max_len, d_model)

        # Create a tensor representing the positions (0, 1, 2, ..., max_len-1)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        # Calculate the division term for the sine and cosine functions.
        # This creates a series of frequencies.
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # Apply sine to even indices in the d_model dimension
        pe[:, 0::2] = torch.sin(position * div_term)
        # Apply cosine to odd indices in the d_model dimension
        pe[:, 1::2] = torch.cos(position * div_term)

        # Add a batch dimension and register 'pe' as a buffer. A buffer is part
        # of the model's state but is not considered a parameter to be trained.
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        """
        Args:
            x (torch.Tensor): The input tensor of shape (batch_size, seq_len, d_model).

        Returns:
            torch.Tensor: The input tensor with added positional encodings.
        """
        # Add the positional encoding to the input tensor.
        # x.size(1) is the sequence length.
        x = x + self.pe[:, :x.size(1), :]
        return x

# ======================================================================================
# 2. The Complete Transformer Model
# ======================================================================================
# This class combines the embedding layer, positional encoding, and the core
# PyTorch nn.Transformer module into a complete sequence-to-sequence model.

class Seq2SeqTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward):
        """
        Args:
            vocab_size (int): The size of the vocabulary (number of unique tokens).
            d_model (int): The dimensionality of the model's embeddings.
            nhead (int): The number of heads in the multi-head attention mechanism.
            num_encoder_layers (int): The number of layers in the encoder.
            num_decoder_layers (int): The number of layers in the decoder.
            dim_feedforward (int): The dimension of the feedforward network model.
        """
        super(Seq2SeqTransformer, self).__init__()

        # Embedding layer: Converts input token indices into dense vectors of d_model size.
        # Think of nn.Embedding as a trainable lookup table.
        # It functions like a dictionary, but its "values" (the embedding vectors) are not constant;
        # they are learned and improved throughout the training process to help the model solve its task.
        #
        # We can also think of nn.Embedding as a process that increases the dimensionality of the input to create a richer representation for the model.
        # This dimension-increasing step is crucial because it takes discrete,
        # categorical inputs (like words or integer tokens) and projects them into a continuous vector space where the model can learn relationships and patterns.

        self.embedding = nn.Embedding(vocab_size, d_model)  # 单个数字被表示成了 d_model 维的向量

        # Positional Encoding layer
        self.pos_encoder = PositionalEncoding(d_model)

        # The core Transformer module from PyTorch
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            batch_first=True  # This makes the input shape (batch, seq, feature)
        )

        # Final linear layer to map the decoder output back to the vocabulary size.
        self.generator = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt):
        """
        Args:
            src (torch.Tensor): The source sequence tensor (batch_size, src_len).
            tgt (torch.Tensor): The target sequence tensor (batch_size, tgt_len).

        Returns:
            torch.Tensor: The output tensor of shape (batch_size, tgt_len, vocab_size).
        """
        # --- Create Masks ---
        # A target mask is needed to prevent the decoder from "seeing" future tokens.
        # This is crucial for auto-regressive decoding.
        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)

        # A source padding mask can be used to ignore padding tokens in the source.
        # For this simple example, we don't have padding, so it's not strictly necessary.
        src_key_padding_mask = (src == 0) # Assuming 0 is the padding token
        tgt_key_padding_mask = (tgt == 0)

        # --- Process Inputs ---
        # 1. Embed the source and target sequences.
        src_emb = self.embedding(src)  # 64 by 10 -> 64 by 10 by 64
        tgt_emb = self.embedding(tgt)

        # 2. Add positional encoding.
        src_emb = self.pos_encoder(src_emb)
        tgt_emb = self.pos_encoder(tgt_emb)

        # 3. Pass the processed sequences through the Transformer.
        output = self.transformer(
            src=src_emb,
            tgt=tgt_emb,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask
        )

        # 4. Pass the output through the final linear layer to get logits.
        return self.generator(output)

# ======================================================================================
# 3. Data Generation and Training
# ======================================================================================

def generate_data(batch_size, seq_len, vocab_size):
    """Generates a batch of data for the 'next number' task."""
    # Generate random sequences of integers
    src = torch.randint(1, vocab_size - 2, (batch_size, seq_len)) # Leave space for padding/special tokens

    # The target is the source sequence shifted by +1
    tgt = src + 1

    # For training, the decoder's input (`tgt_input`) is the target sequence
    # shifted right (with a start token), and the model's output is compared
    # against the original target (`tgt_out`).
    # Let's use 2 as a "start of sequence" token.
    start_token = torch.tensor([[2]] * batch_size)
    tgt_input = torch.cat((start_token, tgt[:, :-1]), dim=1)

    # The ground truth for the loss function is the original target
    tgt_out = tgt

    return src, tgt_input, tgt_out

# --- Hyperparameters ---
VOCAB_SIZE = 20
D_MODEL = 64
NHEAD = 4
NUM_ENCODER_LAYERS = 2
NUM_DECODER_LAYERS = 2
DIM_FEEDFORWARD = 128
SEQ_LEN = 10
BATCH_SIZE = 64
EPOCHS = 500

# --- Main Training Loop ---
if __name__ == "__main__":
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Instantiate the model
    model = Seq2SeqTransformer(
        vocab_size=VOCAB_SIZE,
        d_model=D_MODEL,
        nhead=NHEAD,
        num_encoder_layers=NUM_ENCODER_LAYERS,
        num_decoder_layers=NUM_DECODER_LAYERS,
        dim_feedforward=DIM_FEEDFORWARD
    ).to(device)

    # Loss function and optimizer
    #  the loss is calculated as if the target were one-hot encoded
    criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignore padding token if we had one
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    print("Starting training...")
    for epoch in range(EPOCHS):
        model.train() # Set the model to training mode

        # Generate a batch of data
        src, tgt_input, tgt_out = generate_data(BATCH_SIZE, SEQ_LEN, VOCAB_SIZE)
        src, tgt_input, tgt_out = src.to(device), tgt_input.to(device), tgt_out.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        output = model(src, tgt_input)

        # Reshape for the loss function
        # The output shape is (batch_size, seq_len, vocab_size)
        # The target shape is (batch_size, seq_len)
        # CrossEntropyLoss expects (N, C) and (N)
        # where N is the number of samples and C is the number of classes.
        # It outputs a vector of raw scores (called "logits") for every possible token in the entire vocabulary.
        output_flat = output.view(-1, VOCAB_SIZE)
        tgt_out_flat = tgt_out.view(-1)

        # Calculate loss
        loss = criterion(output_flat, tgt_out_flat)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 50 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}")

    print("Training complete.")

    # ======================================================================================
    # 4. Evaluation / Inference
    # ======================================================================================

    def predict(model, src_sequence):
        """
        Performs inference on a single source sequence.
        This demonstrates auto-regressive decoding.
        """
        model.eval() # Set the model to evaluation mode
        src_sequence = src_sequence.to(device)

        # Start the decoding with the "start of sequence" token (2)
        output_sequence = torch.tensor([[2]], dtype=torch.long).to(device)

        with torch.no_grad():
            for _ in range(len(src_sequence[0])):
                # Get the model's prediction for the next token
                logits = model(src_sequence, output_sequence)

                # Focus on the last token predicted
                last_token_logits = logits[:, -1, :]

                # Find the token with the highest probability (greedy decoding)
                predicted_token = last_token_logits.argmax(1)

                # Append the predicted token to the output sequence
                output_sequence = torch.cat(
                    [output_sequence, predicted_token.unsqueeze(0)], dim=1
                )

        # Remove the start token from the result
        return output_sequence[:, 1:]

    # --- Test the model ---
    test_src, _, test_tgt_out = generate_data(1, SEQ_LEN, VOCAB_SIZE)
    predicted_tgt = predict(model, test_src)

    print("\n--- Inference Example ---")
    print(f"Source Sequence:      {test_src.cpu().numpy().flatten()}")
    print(f"Expected Target:      {test_tgt_out.cpu().numpy().flatten()}")
    print(f"Model Predicted Target: {predicted_tgt.cpu().numpy().flatten()}")