# -*- coding: utf-8 -*-
"""Transformer for Gene Expression Data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xhS5OGmOsOSKOGNqkEtGO9EpDaTENvdZ
"""

import torch
import torch.nn as nn
import math

# The PositionalEncoding class remains the same as it's independent of input type.
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=15000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        # x shape: (batch_size, seq_len, d_model)
        x = x + self.pe[:, :x.size(1), :]
        return x

class GeneExpressionTransformer(nn.Module):
    def __init__(self, num_genes, d_model, nhead, num_encoder_layers, dim_feedforward, num_classes):
        """
        Args:
            num_genes (int): The number of genes in each expression profile.
            d_model (int): The dimensionality of the model's internal representations.
            nhead (int): The number of heads in the multi-head attention mechanism.
            num_encoder_layers (int): The number of layers in the Transformer Encoder.
            dim_feedforward (int): The dimension of the feedforward network model.
            num_classes (int): The number of output classes for classification.
        """
        super(GeneExpressionTransformer, self).__init__()
        self.d_model = d_model

        # 1. Linear Projection Layer (Replaces nn.Embedding)
        # This layer takes each gene's single expression value (in_features=1) and
        # projects it into the model's high-dimensional space (d_model).
        #
        # Think of nn.Linear as a standard, fully-connected neural network layer. It performs a mathematical operation on the input data: output = input * weight + bias.
        #
        # It takes a vector of continuous numbers and transforms it by applying a learned weight matrix and bias.
        # It doesn't care about indices; it cares about the actual floating-point values of the input.
        #
        # All genes share the exact same nn.Linear(1, d_model) projection layer.
        #
        # After every gene's expression value is projected into a d_model vector using the same weights,
        # a unique positional vector is added to each one.
        # This gives the model the crucial information about which gene it is looking at (e.g., gene #1, gene #2, etc.).
        self.linear_projection = nn.Linear(1, d_model)

        # 2. Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model, max_len=num_genes)

        # 3. Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            batch_first=True  # Crucial for (batch, seq, feature) input shape
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)

        # 4. Classification Head
        # We use a special [CLS] token's output for classification.
        # This token will be prepended to each sequence.
        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))

        # The final classifier layer
        self.classifier = nn.Linear(d_model, num_classes)

    def forward(self, src):
        """
        Args:
            src (torch.Tensor): Input tensor of shape (batch_size, num_genes).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, num_classes).
        """
        # Reshape input from (batch_size, num_genes) to (batch_size, num_genes, 1)
        src = src.unsqueeze(-1)

        # Apply the linear projection to each gene's expression value.
        # Shape becomes (batch_size, num_genes, d_model)
        src = self.linear_projection(src)

        # Prepend the [CLS] token to the sequence.
        # The CLS token is a learnable parameter that will aggregate sequence information.
        batch_size = src.size(0)
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        src = torch.cat((cls_tokens, src), dim=1)

        # Add positional encoding. The sequence length is now num_genes + 1.
        src = self.pos_encoder(src)

        # Pass through the Transformer Encoder.
        # Shape remains (batch_size, num_genes + 1, d_model)
        output = self.transformer_encoder(src)

        # Take the output corresponding to the [CLS] token (the first token).
        # Shape is (batch_size, d_model)
        cls_output = output[:, 0, :]

        # Pass through the final classifier.
        # Shape becomes (batch_size, num_classes)
        logits = self.classifier(cls_output)
        return logits


# --- Example Usage ---
if __name__ == '__main__':
    # Hyperparameters
    NUM_GENES = 10000
    NUM_CLASSES = 5  # e.g., 5 different cancer types
    BATCH_SIZE = 32

    D_MODEL = 128      # Internal dimension of the model
    NHEAD = 8          # Number of attention heads
    NUM_LAYERS = 4     # Number of Transformer Encoder layers
    DIM_FEEDFORWARD = 512 # Hidden dimension of the feedforward network

    # Create the model
    model = GeneExpressionTransformer(
        num_genes=NUM_GENES,
        d_model=D_MODEL,
        nhead=NHEAD,
        num_encoder_layers=NUM_LAYERS,
        dim_feedforward=DIM_FEEDFORWARD,
        num_classes=NUM_CLASSES
    )

    # Create a batch of dummy input data (e.g., random floats)
    # Shape: (batch_size, num_genes)
    dummy_input = torch.randn(BATCH_SIZE, NUM_GENES)
    print(f"Shape of input data: {dummy_input.shape}")

    # Get the model's output (logits)
    output_logits = model(dummy_input)

    print(f"Shape of output logits: {output_logits.shape}")
    # Expected output shape: (32, 5) -> (BATCH_SIZE, NUM_CLASSES)